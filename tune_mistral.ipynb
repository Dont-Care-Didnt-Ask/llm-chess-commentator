{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ab2354-9bdc-42c6-9885-bbcb71b58c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=5\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a28f5c-15c5-46d8-b17b-3413addfc385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb79473f-5ce6-4e15-ab15-b9f71adaff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11dbc5d-3058-4eea-a976-b269ffaad921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ba83e37a1646dc8bc446feacbfadab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "modelpath=\"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Load 4-bit quantized model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelpath,    \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    ),\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load (slow) Tokenizer, fast tokenizer sometimes ignores added tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath, use_fast=False)   \n",
    "\n",
    "tokenizer.pad_token = \"</s>\"\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1121923-9701-4ae6-b5d6-1a57722db246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2dfe0-48db-49c1-8f81-785c8ed0c708",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b947ae41-7411-4cdc-9620-3bbe6d637c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_game(game, tokenizer):\n",
    "    inputs = tokenizer(text=game, return_tensors=\"pt\", truncation=True, max_length=512 * 3, pad_to_multiple_of=256)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def add_comment_mask(inputs, opening_bracket_id, closing_bracket_id):\n",
    "    mask = []\n",
    "\n",
    "    is_inside = False\n",
    "    for token in inputs.input_ids[0]:\n",
    "        if token == opening_bracket_id:\n",
    "            mask.append(False)\n",
    "            is_inside = True\n",
    "        elif token == closing_bracket_id:\n",
    "            mask.append(True)\n",
    "            is_inside = False\n",
    "        else:\n",
    "            mask.append(is_inside)\n",
    "\n",
    "    inputs[\"comment_mask\"] = torch.tensor(mask).to(inputs.input_ids.device)\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"].squeeze()\n",
    "    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].squeeze()\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8860eb2f-81e6-4d05-8ad4-e44e626b3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChessCommmentDataset(Dataset):\n",
    "    def __init__(self, path_to_pgn: str, tokenizer):\n",
    "        \"\"\"\n",
    "        A dataset of chess games with comments. Each sample is a game in PGN format with textual comments.\n",
    "\n",
    "        path_to_pgn: str\n",
    "            Path to a single pgn with all games.\n",
    "        tokenizer:\n",
    "            Tokenizer used to convert all strings into tokens.\n",
    "        \"\"\"\n",
    "        self.path_to_pgn = path_to_pgn\n",
    "        self.game_offsets = []\n",
    "        self.games = []\n",
    "\n",
    "        with open(path_to_pgn) as pgn:\n",
    "            while True:\n",
    "                offset = pgn.tell()\n",
    "                game = chess.pgn.read_game(pgn)\n",
    "\n",
    "                if game is None:\n",
    "                    # End of the file (or error)\n",
    "                    print(f\"Scanned {len(self.game_offsets)} games\")\n",
    "                    break\n",
    "\n",
    "                self.game_offsets.append(offset)\n",
    "                self.games.append(str(game.mainline()) + \" Result: \" + str(game.headers[\"Result\"]))\n",
    "\n",
    "        closing_bracket_id = tokenizer(\"}\", add_special_tokens=False).input_ids[0]\n",
    "        opening_bracket_id = tokenizer(\"{\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "        self.games = [\n",
    "            add_comment_mask(tokenize_game(game, tokenizer), opening_bracket_id, closing_bracket_id) for game in self.games\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.games[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.game_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9f9d59-8849-44f8-bf5e-d8f7bef2d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned 500 games\n"
     ]
    }
   ],
   "source": [
    "dataset = ChessCommmentDataset(\"data/famous_games.pgn\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a67f06-3db8-4235-9a15-f9a1816417eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:      1  Wordpiece: <s>    Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28740  Wordpiece: 1      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    317  Wordpiece: e      Is comment: False\n",
      "Token:  28781  Wordpiece: 4      Is comment: False\n",
      "Token:    317  Wordpiece: e      Is comment: False\n",
      "Token:  28782  Wordpiece: 5      Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28750  Wordpiece: 2      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    418  Wordpiece: N      Is comment: False\n",
      "Token:  28722  Wordpiece: f      Is comment: False\n",
      "Token:  28770  Wordpiece: 3      Is comment: False\n",
      "Token:    418  Wordpiece: N      Is comment: False\n",
      "Token:  28717  Wordpiece: c      Is comment: False\n",
      "Token:  28784  Wordpiece: 6      Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28770  Wordpiece: 3      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    365  Wordpiece: B      Is comment: False\n",
      "Token:  28717  Wordpiece: c      Is comment: False\n",
      "Token:  28781  Wordpiece: 4      Is comment: False\n",
      "Token:    365  Wordpiece: B      Is comment: False\n",
      "Token:  28717  Wordpiece: c      Is comment: False\n",
      "Token:  28782  Wordpiece: 5      Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28781  Wordpiece: 4      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    451  Wordpiece: O      Is comment: False\n",
      "Token:  28733  Wordpiece: -      Is comment: False\n",
      "Token:  28762  Wordpiece: O      Is comment: False\n",
      "Token:    285  Wordpiece: f      Is comment: False\n",
      "Token:  28782  Wordpiece: 5      Is comment: False\n",
      "Token:    371  Wordpiece: {      Is comment: False\n",
      "Token:  21960  Wordpiece: Aw     Is comment: True\n",
      "Token:   1007  Wordpiece: ful    Is comment: True\n",
      "Token:  28723  Wordpiece: .      Is comment: True\n",
      "Token:   4777  Wordpiece: Black  Is comment: True\n",
      "Token:    439  Wordpiece: ex     Is comment: True\n",
      "Token:   9339  Wordpiece: poses  Is comment: True\n",
      "Token:    516  Wordpiece: his    Is comment: True\n",
      "Token:  27410  Wordpiece: kings  Is comment: True\n",
      "Token:    547  Wordpiece: ide    Is comment: True\n",
      "Token:    304  Wordpiece: and    Is comment: True\n",
      "Token:  15706  Wordpiece: opens  Is comment: True\n",
      "Token:    272  Wordpiece: the    Is comment: True\n",
      "Token:   2039  Wordpiece: game   Is comment: True\n",
      "Token:    739  Wordpiece: when   Is comment: True\n",
      "Token:    400  Wordpiece: he     Is comment: True\n",
      "Token:    349  Wordpiece: is     Is comment: True\n",
      "Token:   2910  Wordpiece: behind  Is comment: True\n",
      "Token:    297  Wordpiece: in     Is comment: True\n",
      "Token:   4099  Wordpiece: development  Is comment: True\n",
      "Token:  28723  Wordpiece: .      Is comment: True\n",
      "Token:    443  Wordpiece: }      Is comment: True\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28782  Wordpiece: 5      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    281  Wordpiece: d      Is comment: False\n",
      "Token:  28781  Wordpiece: 4      Is comment: False\n",
      "Token:    439  Wordpiece: ex     Is comment: False\n",
      "Token:  28715  Wordpiece: d      Is comment: False\n",
      "Token:  28781  Wordpiece: 4      Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28784  Wordpiece: 6      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    317  Wordpiece: e      Is comment: False\n",
      "Token:  28782  Wordpiece: 5      Is comment: False\n",
      "Token:    371  Wordpiece: {      Is comment: False\n",
      "Token:    851  Wordpiece: This   Is comment: True\n",
      "Token:   5881  Wordpiece: pa     Is comment: True\n",
      "Token:    880  Wordpiece: wn     Is comment: True\n",
      "Token:    349  Wordpiece: is     Is comment: True\n",
      "Token:    264  Wordpiece: a      Is comment: True\n",
      "Token:  14056  Wordpiece: bone   Is comment: True\n",
      "Token:    297  Wordpiece: in     Is comment: True\n",
      "Token:   4777  Wordpiece: Black  Is comment: True\n",
      "Token:  28742  Wordpiece: '      Is comment: True\n",
      "Token:  28713  Wordpiece: s      Is comment: True\n",
      "Token:  10807  Wordpiece: throat  Is comment: True\n",
      "Token:  28725  Wordpiece: ,      Is comment: True\n",
      "Token:    304  Wordpiece: and    Is comment: True\n",
      "Token:    513  Wordpiece: if     Is comment: True\n",
      "Token:    400  Wordpiece: he     Is comment: True\n",
      "Token:   2368  Wordpiece: doesn  Is comment: True\n",
      "Token:  28742  Wordpiece: '      Is comment: True\n",
      "Token:  28707  Wordpiece: t      Is comment: True\n",
      "Token:    511  Wordpiece: do     Is comment: True\n",
      "Token:   1545  Wordpiece: something  Is comment: True\n",
      "Token:    684  Wordpiece: about  Is comment: True\n",
      "Token:    378  Wordpiece: it     Is comment: True\n",
      "Token:   5673  Wordpiece: White  Is comment: True\n",
      "Token:    622  Wordpiece: will   Is comment: True\n",
      "Token:   1721  Wordpiece: break  Is comment: True\n",
      "Token:   1059  Wordpiece: through  Is comment: True\n",
      "Token:    297  Wordpiece: in     Is comment: True\n",
      "Token:    272  Wordpiece: the    Is comment: True\n",
      "Token:   4982  Wordpiece: center  Is comment: True\n",
      "Token:    304  Wordpiece: and    Is comment: True\n",
      "Token:  12573  Wordpiece: explo  Is comment: True\n",
      "Token:    279  Wordpiece: it     Is comment: True\n",
      "Token:   4777  Wordpiece: Black  Is comment: True\n",
      "Token:  28742  Wordpiece: '      Is comment: True\n",
      "Token:  28713  Wordpiece: s      Is comment: True\n",
      "Token:    297  Wordpiece: in     Is comment: True\n",
      "Token:   2437  Wordpiece: ability  Is comment: True\n",
      "Token:    298  Wordpiece: to     Is comment: True\n",
      "Token:  19007  Wordpiece: castle  Is comment: True\n",
      "Token:   2485  Wordpiece: short  Is comment: True\n",
      "Token:  28723  Wordpiece: .      Is comment: True\n",
      "Token:    443  Wordpiece: }      Is comment: True\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28784  Wordpiece: 6      Is comment: False\n",
      "Token:   1101  Wordpiece: ...    Is comment: False\n",
      "Token:    281  Wordpiece: d      Is comment: False\n",
      "Token:  28784  Wordpiece: 6      Is comment: False\n",
      "Token:    371  Wordpiece: {      Is comment: False\n",
      "Token:   1092  Wordpiece: But    Is comment: True\n",
      "Token:    456  Wordpiece: this   Is comment: True\n",
      "Token:  15706  Wordpiece: opens  Is comment: True\n",
      "Token:   1722  Wordpiece: things  Is comment: True\n",
      "Token:   1019  Wordpiece: even   Is comment: True\n",
      "Token:   3629  Wordpiece: further  Is comment: True\n",
      "Token:    395  Wordpiece: with   Is comment: True\n",
      "Token:   4777  Wordpiece: Black  Is comment: True\n",
      "Token:   2461  Wordpiece: having  Is comment: True\n",
      "Token:    708  Wordpiece: no     Is comment: True\n",
      "Token:   4676  Wordpiece: chance  Is comment: True\n",
      "Token:    298  Wordpiece: to     Is comment: True\n",
      "Token:    625  Wordpiece: get    Is comment: True\n",
      "Token:    516  Wordpiece: his    Is comment: True\n",
      "Token:   6779  Wordpiece: king   Is comment: True\n",
      "Token:    575  Wordpiece: out    Is comment: True\n",
      "Token:    302  Wordpiece: of     Is comment: True\n",
      "Token:    272  Wordpiece: the    Is comment: True\n",
      "Token:   1069  Wordpiece: way    Is comment: True\n",
      "Token:  28723  Wordpiece: .      Is comment: True\n",
      "Token:    443  Wordpiece: }      Is comment: True\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28787  Wordpiece: 7      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    439  Wordpiece: ex     Is comment: False\n",
      "Token:  28715  Wordpiece: d      Is comment: False\n",
      "Token:  28784  Wordpiece: 6      Is comment: False\n",
      "Token:   1186  Wordpiece: Q      Is comment: False\n",
      "Token:  10044  Wordpiece: xd     Is comment: False\n",
      "Token:  28784  Wordpiece: 6      Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28783  Wordpiece: 8      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:   1298  Wordpiece: Re     Is comment: False\n",
      "Token:  28740  Wordpiece: 1      Is comment: False\n",
      "Token:  28806  Wordpiece: +      Is comment: False\n",
      "Token:    371  Wordpiece: {      Is comment: False\n",
      "Token:   5673  Wordpiece: White  Is comment: True\n",
      "Token:   4207  Wordpiece: immedi  Is comment: True\n",
      "Token:    270  Wordpiece: at     Is comment: True\n",
      "Token:    639  Wordpiece: el     Is comment: True\n",
      "Token:  28724  Wordpiece: y      Is comment: True\n",
      "Token:  12573  Wordpiece: explo  Is comment: True\n",
      "Token:   1046  Wordpiece: its    Is comment: True\n",
      "Token:    272  Wordpiece: the    Is comment: True\n",
      "Token:   1565  Wordpiece: open   Is comment: True\n",
      "Token:    317  Wordpiece: e      Is comment: True\n",
      "Token:  28733  Wordpiece: -      Is comment: True\n",
      "Token:   1516  Wordpiece: file   Is comment: True\n",
      "Token:    304  Wordpiece: and    Is comment: True\n",
      "Token:   1304  Wordpiece: inv    Is comment: True\n",
      "Token:   3387  Wordpiece: ites   Is comment: True\n",
      "Token:   4777  Wordpiece: Black  Is comment: True\n",
      "Token:    298  Wordpiece: to     Is comment: True\n",
      "Token:   2318  Wordpiece: move   Is comment: True\n",
      "Token:    778  Wordpiece: into   Is comment: True\n",
      "Token:    264  Wordpiece: a      Is comment: True\n",
      "Token:  26092  Wordpiece: nasty  Is comment: True\n",
      "Token:   9550  Wordpiece: pin    Is comment: True\n",
      "Token:  28723  Wordpiece: .      Is comment: True\n",
      "Token:    443  Wordpiece: }      Is comment: True\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28783  Wordpiece: 8      Is comment: False\n",
      "Token:   1101  Wordpiece: ...    Is comment: False\n",
      "Token:    418  Wordpiece: N      Is comment: False\n",
      "Token:    490  Wordpiece: ge     Is comment: False\n",
      "Token:  28787  Wordpiece: 7      Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28774  Wordpiece: 9      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:  27652  Wordpiece: Ng     Is comment: False\n",
      "Token:  28782  Wordpiece: 5      Is comment: False\n",
      "Token:   3147  Wordpiece: Ne     Is comment: False\n",
      "Token:  28782  Wordpiece: 5      Is comment: False\n",
      "Token:    371  Wordpiece: {      Is comment: False\n",
      "Token:    935  Wordpiece: Un     Is comment: True\n",
      "Token:  10002  Wordpiece: pin    Is comment: True\n",
      "Token:    971  Wordpiece: ning   Is comment: True\n",
      "Token:    272  Wordpiece: the    Is comment: True\n",
      "Token:    747  Wordpiece: kn     Is comment: True\n",
      "Token:    454  Wordpiece: ight   Is comment: True\n",
      "Token:    356  Wordpiece: on     Is comment: True\n",
      "Token:    317  Wordpiece: e      Is comment: True\n",
      "Token:  28787  Wordpiece: 7      Is comment: True\n",
      "Token:  28725  Wordpiece: ,      Is comment: True\n",
      "Token:    562  Wordpiece: but    Is comment: True\n",
      "Token:   7312  Wordpiece: walking  Is comment: True\n",
      "Token:    778  Wordpiece: into   Is comment: True\n",
      "Token:    298  Wordpiece: to     Is comment: True\n",
      "Token:    284  Wordpiece: p      Is comment: True\n",
      "Token:   1126  Wordpiece: ins    Is comment: True\n",
      "Token:    356  Wordpiece: on     Is comment: True\n",
      "Token:    317  Wordpiece: e      Is comment: True\n",
      "Token:  28782  Wordpiece: 5      Is comment: True\n",
      "Token:  28723  Wordpiece: .      Is comment: True\n",
      "Token:    443  Wordpiece: }      Is comment: True\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28740  Wordpiece: 1      Is comment: False\n",
      "Token:  28734  Wordpiece: 0      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    365  Wordpiece: B      Is comment: False\n",
      "Token:  28722  Wordpiece: f      Is comment: False\n",
      "Token:  28781  Wordpiece: 4      Is comment: False\n",
      "Token:    418  Wordpiece: N      Is comment: False\n",
      "Token:  28787  Wordpiece: 7      Is comment: False\n",
      "Token:  28721  Wordpiece: g      Is comment: False\n",
      "Token:  28784  Wordpiece: 6      Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28740  Wordpiece: 1      Is comment: False\n",
      "Token:  28740  Wordpiece: 1      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    365  Wordpiece: B      Is comment: False\n",
      "Token:   8141  Wordpiece: xe     Is comment: False\n",
      "Token:  28782  Wordpiece: 5      Is comment: False\n",
      "Token:    418  Wordpiece: N      Is comment: False\n",
      "Token:   8141  Wordpiece: xe     Is comment: False\n",
      "Token:  28782  Wordpiece: 5      Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28740  Wordpiece: 1      Is comment: False\n",
      "Token:  28750  Wordpiece: 2      Is comment: False\n",
      "Token:  28723  Wordpiece: .      Is comment: False\n",
      "Token:    418  Wordpiece: N      Is comment: False\n",
      "Token:  28722  Wordpiece: f      Is comment: False\n",
      "Token:  28787  Wordpiece: 7      Is comment: False\n",
      "Token:    371  Wordpiece: {      Is comment: False\n",
      "Token:    415  Wordpiece: The    Is comment: True\n",
      "Token:   9550  Wordpiece: pin    Is comment: True\n",
      "Token:   1055  Wordpiece: now    Is comment: True\n",
      "Token:    307  Wordpiece: n      Is comment: True\n",
      "Token:   1468  Wordpiece: ets    Is comment: True\n",
      "Token:    438  Wordpiece: at     Is comment: True\n",
      "Token:   2429  Wordpiece: least  Is comment: True\n",
      "Token:    264  Wordpiece: a      Is comment: True\n",
      "Token:   5511  Wordpiece: piece  Is comment: True\n",
      "Token:  28725  Wordpiece: ,      Is comment: True\n",
      "Token:    395  Wordpiece: with   Is comment: True\n",
      "Token:   4777  Wordpiece: Black  Is comment: True\n",
      "Token:  28742  Wordpiece: '      Is comment: True\n",
      "Token:  28713  Wordpiece: s      Is comment: True\n",
      "Token:   6779  Wordpiece: king   Is comment: True\n",
      "Token:   1309  Wordpiece: still  Is comment: True\n",
      "Token:  10231  Wordpiece: stuck  Is comment: True\n",
      "Token:    297  Wordpiece: in     Is comment: True\n",
      "Token:    272  Wordpiece: the    Is comment: True\n",
      "Token:   4986  Wordpiece: middle  Is comment: True\n",
      "Token:    842  Wordpiece: .      Is comment: True\n",
      "Token:    443  Wordpiece: }      Is comment: True\n",
      "Token:   6760  Wordpiece: Result  Is comment: False\n",
      "Token:  28747  Wordpiece: :      Is comment: False\n",
      "Token:  28705  Wordpiece:        Is comment: False\n",
      "Token:  28740  Wordpiece: 1      Is comment: False\n",
      "Token:  28733  Wordpiece: -      Is comment: False\n",
      "Token:  28734  Wordpiece: 0      Is comment: False\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[15]\n",
    "\n",
    "for i in range(sample.input_ids.shape[-1]):\n",
    "    wordpiece = tokenizer.decode(sample.input_ids[i], add_special_tokens=False)\n",
    "    \n",
    "    print(f\"Token: {sample.input_ids[i].item():6}  Wordpiece: {wordpiece:5}  Is comment: {sample.comment_mask[i].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0868c2-67e2-4a3e-8d89-aab89ffd8222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = [sample.input_ids.shape[-1] for sample in dataset]\n",
    "plt.hist(lengths, bins=30)\n",
    "plt.title(\"Games length distribution (in tokens)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a685e44a-0c05-48c3-ad02-adfb2d767be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out what we should use as padding value\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc003c50-5110-402e-898b-8a1d4c049bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collator(examples):\n",
    "    \"\"\"\n",
    "    Takes a list of dicts, returns a dict of padded tensors. \n",
    "    \"\"\"\n",
    "    batch_keys = examples[0].keys()\n",
    "            \n",
    "    return {key: pad_sequence([sample[key] for sample in examples], batch_first=True, padding_value=2 if key == \"input_ids\" else 0)\n",
    "            for key in batch_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b240097e-e04c-43f7-aa56-be18e3cf6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = collator(\n",
    "    [dataset[0], dataset[1], dataset[2]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a79e47-7265-4678-8668-49fe5f2a04b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 28705, 28740,  ..., 28734, 28733, 28740],\n",
       "         [    1, 28705, 28740,  ...,     2,     2,     2],\n",
       "         [    1, 28705, 28740,  ...,     2,     2,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'comment_mask': tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b1ca5-efa1-4fd3-b968-8366528f7b98",
   "metadata": {},
   "source": [
    "## Configure training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6058ff9-8ab4-48b9-879f-789285743fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters to model\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "config = LoraConfig(\n",
    "    r=4, \n",
    "    lora_alpha=16, \n",
    "    target_modules = ['q_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj'],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    #modules_to_save = [\"lm_head\", \"embed_tokens\"],\t\t# needed because we added new tokens to tokenizer/model\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4118f55-00df-44cd-bae5-34c6f2c139d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"input_ids\")\n",
    "        comment_mask = inputs.pop(\"comment_mask\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        # loss = (criterion(outputs.logits.transpose(1,2), labels) * comment_mask).sum() / (comment_mask.sum() + 1e-3)\n",
    "        criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        loss = criterion(outputs.logits.transpose(1,2), labels) \n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e91e4b8-3545-4317-9e6e-bb1dedfee87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "epochs = 1\n",
    "gradient_accumulation_steps = 1\n",
    "steps_per_epoch = len(dataset) // (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    #eval_steps=steps_per_epoch,\t\t# eval and save once per epoch  \t\n",
    "    save_steps=steps_per_epoch,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-6,\n",
    "    group_by_length=True,\n",
    "    fp16=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4838a20f-df0d-4b40-8329-e720986d13ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seleznyov/micromamba/envs/stein/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 08:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.731100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>13.938700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>13.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>13.735400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>13.543100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>13.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>13.374900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>13.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>13.274900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>12.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>13.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>12.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>12.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>13.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>13.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>12.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>12.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>12.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>12.931700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>12.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>12.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>12.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>12.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>12.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>12.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>12.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>12.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>12.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>12.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>12.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>12.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>12.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>12.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>11.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>12.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>11.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>12.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>11.487700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>11.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>11.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>11.727800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>11.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>11.466600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>10.872200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.783200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>11.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>11.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>11.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>11.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>11.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>11.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>11.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>11.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>11.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>10.992600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>10.775800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>11.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>10.755200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>10.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>10.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>10.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>10.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>10.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>10.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>10.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>10.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>10.520500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>10.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>9.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>9.668200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>9.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>10.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>9.603700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>9.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>9.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>9.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>9.339400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>9.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>8.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>8.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>8.938100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>8.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>8.832100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>8.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>8.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>8.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>8.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>8.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>8.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>8.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>8.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>8.408700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>8.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>7.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>8.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>7.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>8.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>7.678200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>7.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>7.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>7.503500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>7.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>7.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>7.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>7.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>6.824800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>7.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>6.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>6.793100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>6.706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>6.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>6.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>6.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>6.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>6.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>5.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>5.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.931200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>5.615200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>5.614400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>5.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>5.665600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>4.880100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>5.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>5.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>5.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>4.905800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>4.568700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>4.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>5.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>4.821700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>4.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>4.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>4.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>4.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>4.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>4.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>4.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>4.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>4.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>4.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>4.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>4.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>4.390200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>4.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>3.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>4.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>4.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>4.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.468300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>3.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>3.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>3.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>3.588200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>3.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>3.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>3.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>3.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>2.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>2.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>2.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.327900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>2.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>2.680200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>2.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.832200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>3.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>2.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>2.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>2.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>2.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>2.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>2.564300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>2.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>2.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.646300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>2.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>2.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>2.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.875300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.880600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>1.609900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>1.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>1.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>1.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>1.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>1.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>1.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>1.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>1.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.956900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>1.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.704600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>1.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.818700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.562400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.569700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.515500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.409600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.835200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>1.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.499700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=6.445546573042869, metrics={'train_runtime': 506.4439, 'train_samples_per_second': 0.987, 'train_steps_per_second': 0.494, 'total_flos': 1.4811356719644672e+16, 'train_loss': 6.445546573042869, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "#trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset,\n",
    "    #eval_dataset=dataset_tokenized[\"test\"],\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d309c3-0450-437d-aca2-6aedbf02f9e6",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37607138-3f0a-4dfd-80a8-e2088eef5240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closing_bracket_id = tokenizer(\"}\", add_special_tokens=False).input_ids[0]\n",
    "closing_bracket_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36f6cb50-e257-46e1-839c-05da34d7f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def comment_chess_game(move_sequence, prompt, model, tokenizer, device, max_comment_size=40):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        move_sequence: List[str], \n",
    "            A sequence of moves from White and Black in PGN format. Moves for White also include the move number. Comments and NAGs are excluded.\n",
    "            example: [\"1. e4 \", \"e5 \", \"2. Qh5 \", \"Nc6 \", \"3. Bc4 \", \"Nf6 \", \"4. Qxf7#\"]\n",
    "        prompt: str\n",
    "            Prompt used to incentivize the model to provide coherent comments\n",
    "        model: transformers.AutoModelForCausalLM\n",
    "            A model to generate comments (e.g. Mistral 7B).\n",
    "        tokenizer: transformers.AutoTokenizer\n",
    "            Corresponing tokenizer\n",
    "\n",
    "    Output:\n",
    "        commented_game: str\n",
    "            A single string with all moves and occasional comments, hopefully in PGN format.\n",
    "    \"\"\"\n",
    "    prefix = prompt\n",
    "    game = prompt\n",
    "\n",
    "    for move in tqdm(move_sequence, desc=\"Moves\"):\n",
    "        prefix += move\n",
    "        # to initialize a comment\n",
    "        prefix += \"{ \"\n",
    "\n",
    "        print(move)\n",
    "        \n",
    "        inputs = tokenizer(prefix, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(**inputs, eos_token_id=closing_bracket_id, pad_token_id=closing_bracket_id, min_new_tokens=max_comment_size//4, max_new_tokens=max_comment_size)\n",
    "        comment = tokenizer.decode(output[0, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "        prefix += comment\n",
    "        print(\"Comment:\", comment)\n",
    "        print(\"=\" * 25)\n",
    "\n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18834b00-5217-4dd3-a215-fec6fe844827",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Several chess games with concise comments, which briefly explain the upsides and downsides of the move \\\n",
    "(e.g. whether it develops a piece, wins material, brings king to safety, puts a piece to a better/worse square, creates a threat, fails to pary on opponents idea).\n",
    "Comment is as always succint as possible -- e.g. { Develops a knight and controls the center. }, or { Wins a pawn, but loses a tempo. } or { Creates a checkmating threat. }.\n",
    "Comment always focuses on pros and cons of the move, or on the idea/reasoning behind it.\n",
    "Each comment always starts with an opening curly bracket '{' and ends with a closing curly bracket '}'.\n",
    "\n",
    "Game 1:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d9a7ab0-0745-4f3f-87c0-59d19cff9b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da9463d68874fbd889d3bdba866ac54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Moves:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. f3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seleznyov/micromamba/envs/stein/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/seleznyov/micromamba/envs/stein/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/seleznyov/micromamba/envs/stein/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment:                                       \n",
      "=========================\n",
      " e5 \n",
      "Comment:                                       \n",
      "=========================\n",
      " 2. g4 \n",
      "Comment:                                       \n",
      "=========================\n",
      " Qh4# \n",
      "Comment:                                       \n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "game = [\"1. f3 \", \" e5 \", \" 2. g4 \", \" Qh4# \"]\n",
    "\n",
    "commented_game = comment_chess_game(game, prompt, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ccb173e-e8aa-4460-b2bf-877216dbba4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371e5aa5d84943e3a093c3ff37cd0468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Moves:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. e4 \n",
      "Comment:                                       \n",
      "=========================\n",
      " e5 \n",
      "Comment:                                       \n",
      "=========================\n",
      " 2. Qh5 \n",
      "Comment:                                       \n",
      "=========================\n",
      " Nc6 \n",
      "Comment:                                       \n",
      "=========================\n",
      " 3. Bc4 \n",
      "Comment:                                       \n",
      "=========================\n",
      " Nf6 \n",
      "Comment:                                       \n",
      "=========================\n",
      "4. Qxf7# \n",
      "Comment:                                       \n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "game = [\"1. e4 \", \" e5 \", \" 2. Qh5 \", \" Nc6 \", \" 3. Bc4 \", \" Nf6 \", \"4. Qxf7# \"]\n",
    "\n",
    "commented_game = comment_chess_game(game, prompt, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ebbe2-9479-4ff0-b83c-04ca4b88f0de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
